// Generated by CoffeeScript 1.6.3
(function() {
  var FS, Q, USAGE, argv, batchUp, max, min, partition, positions, regionFilePattern, since, timeSeries, _,
    _this = this;

  FS = require("q-io/fs");

  Q = require("q");

  _ = require("underscore")._;

  argv = require('optimist').argv;

  USAGE = "USAGE: " + process.argv[0] + " " + process.argv[1] + " --dir <directory to process> --since <millis since epoch> --out <file to output time series in>";

  if (!((argv.dir != null) && argv.out && argv.since)) {
    console.error(USAGE);
    process.exit(1);
  }

  since = parseInt(argv.since);

  partition = function(partitionSize) {
    return function(list) {
      var groups;
      groups = _.groupBy(list, function(value, index) {
        return Math.floor(index / partitionSize);
      });
      return _.map(groups, function(value) {
        return value;
      });
    };
  };

  batchUp = partition(100);

  regionFilePattern = /.+\/(\d+)\/regions$/;

  positions = {};

  timeSeries = {};

  max = Number.MIN_VALUE;

  min = Number.MAX_VALUE;

  FS.listTree(argv.dir).then(function(entries) {
    var batches, dispatchBatch, reads, regionFileNames;
    regionFileNames = _.filter(entries, function(e) {
      return regionFilePattern.test(e);
    });
    batches = batchUp(regionFileNames);
    console.log("Files to read: " + regionFileNames.length);
    console.log("Batches: " + batches.length);
    reads = function(fileNames) {
      return _.map(fileNames, function(f) {
        return FS.read(f).then(function(content) {
          var e;
          try {
            return {
              timestamp: parseInt(f.match(regionFilePattern)[1]),
              content: JSON.parse(content)
            };
          } catch (_error) {
            e = _error;
            console.warn("Ignoring " + f + ": " + e);
            return null;
          }
        });
      });
    };
    dispatchBatch = function(index, batches) {
      var batch;
      if (index < batches.length) {
        batch = batches[index];
        console.log("Doing batch " + (index + 1) + "/" + batches.length + " of length " + batch.length);
        return Q.all(reads(batch)).then(function(filesRead) {
          var entry, fileRead, tweets, tweetsAtTimestamp, _i, _j, _len, _len1, _ref;
          for (_i = 0, _len = filesRead.length; _i < _len; _i++) {
            fileRead = filesRead[_i];
            if (fileRead != null) {
              _ref = fileRead.content;
              for (_j = 0, _len1 = _ref.length; _j < _len1; _j++) {
                entry = _ref[_j];
                if (fileRead.timestamp >= since) {
                  positions[entry.name] = entry.geo;
                  tweetsAtTimestamp = timeSeries[fileRead.timestamp];
                  if (tweetsAtTimestamp == null) {
                    tweetsAtTimestamp = timeSeries[fileRead.timestamp] = {};
                  }
                  tweets = entry.summary.tweets;
                  tweetsAtTimestamp[entry.name] = tweets;
                  min = Math.min(min, tweets);
                  max = Math.max(max, tweets);
                }
              }
            }
          }
          return dispatchBatch(index + 1, batches);
        }, function(error) {
          return console.error(error);
        });
      }
    };
    return dispatchBatch(0, batches);
  }).then(function() {
    var name, out, timestamp, tweets, tweetsAtTimestamp;
    out = {
      range: {
        min: min,
        max: max
      },
      data: (function() {
        var _results;
        _results = [];
        for (timestamp in timeSeries) {
          tweetsAtTimestamp = timeSeries[timestamp];
          _results.push({
            timestamp: timestamp,
            snapshot: (function() {
              var _results1;
              _results1 = [];
              for (name in tweetsAtTimestamp) {
                tweets = tweetsAtTimestamp[name];
                _results1.push({
                  name: name,
                  geo: positions[name],
                  summary: {
                    tweets: tweets
                  }
                });
              }
              return _results1;
            })()
          });
        }
        return _results;
      })()
    };
    return FS.write(argv.out, JSON.stringify(out)).then(function() {
      return console.log("Wrote entries to " + argv.out);
    });
  }).fail(function(error) {
    return console.error(error);
  });

}).call(this);

/*
//@ sourceMappingURL=extractTimeSeries.map
*/
